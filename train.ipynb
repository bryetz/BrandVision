{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1bff10b6-0845-4f0e-a6ac-75b66c2e686b",
      "metadata": {
        "id": "1bff10b6-0845-4f0e-a6ac-75b66c2e686b"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import keras\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras import layers\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50\n",
        "from keras.models import load_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa4b005a-7975-49a5-975f-d2e17f0460e7",
      "metadata": {
        "id": "aa4b005a-7975-49a5-975f-d2e17f0460e7",
        "outputId": "c971d53b-d586-4816-e706-ce430d7da215"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(5933, 300, 300, 3)\n"
          ]
        }
      ],
      "source": [
        "# Step 1. Load the training data rom the .npy files\n",
        "data = np.load('data_train.npy')\n",
        "\n",
        "# Step 2. Load the corresponding labels from the .npy files\n",
        "labels = np.load('labels_train_corrected.npy')\n",
        "\n",
        "# Transpose the data matrix\n",
        "data = data.T\n",
        "\n",
        "# Reshape the data into a 4D array, assuming images of size 300x300 with 3 color channels (RGB)\n",
        "data = data[:,:].reshape(data.shape[0],300,300,3)\n",
        "print(data.shape) # Print the shape of the reshaped data\n",
        "\n",
        "# Split the data into training and test sets, with 30% of the data reserved for testing\n",
        "x_train_full, x_test, t_train_full, t_test = train_test_split(data, labels, test_size=0.3, random_state=0, shuffle=True)\n",
        "\n",
        "# Further split the training data into training and validation sets, with 20% of the training data reserved for validation\n",
        "x_train, x_valid, t_train, t_valid = train_test_split(x_train_full, t_train_full, test_size=0.2)\n",
        "\n",
        "# Save the test data and corresponding labels to .npy files\n",
        "np.save('x_test.npy', x_test)\n",
        "np.save('t_test.npy', t_test)\n",
        "\n",
        "# Delete variables to free up memory\n",
        "del data\n",
        "del labels\n",
        "del x_train_full\n",
        "del t_train_full"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03046518",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 3. specify 'checkpoint_path' as the path to the saved model file \"best_model.h5\"\n",
        "checkpoint_path = \"best_model.h5\"\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d092571-7704-4e97-b21e-a8d51fb444db",
      "metadata": {
        "id": "9d092571-7704-4e97-b21e-a8d51fb444db"
      },
      "outputs": [],
      "source": [
        "# Convert the training and validation data to 'float16' type and normalize the pixel values to the range [0, 1] by dividing by 255\n",
        "x_train = x_train.astype('float16') / 255\n",
        "x_valid = x_valid.astype('float16') / 255\n",
        "\n",
        "# Convert the training and validation labels to one-hot encoded format with 10 classes\n",
        "t_train = tf.keras.utils.to_categorical(t_train, num_classes=10)\n",
        "t_valid = tf.keras.utils.to_categorical(t_valid, num_classes=10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "788bb376-a47b-41ea-b33c-7709d3f3e195",
      "metadata": {
        "id": "788bb376-a47b-41ea-b33c-7709d3f3e195",
        "outputId": "f8d58027-e9bd-40be-8373-3fce9e6dd76e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-08-07 16:56:21.240335: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-08-07 16:56:21.751345: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 79111 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:47:00.0, compute capability: 8.0\n"
          ]
        }
      ],
      "source": [
        "# Create a ResNet50 base model with pre-trained weights from ImageNet, without the top (output) layer\n",
        "# The input shape is set to (300, 300, 3) to match the shape of the images\n",
        "base_model = ResNet50(input_shape=(300,300,3), include_top=False, weights='imagenet')\n",
        "\n",
        "# Set the base model to be trainable, allowing its weights to be updated during training\n",
        "base_model.trainable = True\n",
        "\n",
        "# Start building the custom model on top of the base model\n",
        "x = base_model.output\n",
        "\n",
        "# Apply random horizontal and vertical flipping as a data augmentation technique\n",
        "x = tf.keras.layers.RandomFlip(\"horizontal_and_vertical\")(x)\n",
        "\n",
        "# Apply random rotation to the images, with a maximum rotation of 0.2 radians\n",
        "x = tf.keras.layers.RandomRotation(0.2)(x)\n",
        "\n",
        "# Apply global average pooling to reduce the spatial dimensions of the feature maps\n",
        "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "\n",
        "# Apply a dropout layer with a rate of 0.4 to help prevent overfitting\n",
        "x = tf.keras.layers.Dropout(0.4)(x)\n",
        "\n",
        "# Add the final dense layer with 10 output units (for 10 classes) and a softmax activation function\n",
        "outputs = tf.keras.layers.Dense(10, activation='softmax')(x)\n",
        "\n",
        "# Create the final model by connecting the base model's input to the custom output layer\n",
        "model = tf.keras.Model(inputs=base_model.input, outputs=outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94e6b20c-91a9-47bf-80dd-d7f21dc43ae6",
      "metadata": {
        "id": "94e6b20c-91a9-47bf-80dd-d7f21dc43ae6"
      },
      "outputs": [],
      "source": [
        "# Step 4. Load the previously saved best model (if there is one, otherwise comment this line out)\n",
        "model = load_model(checkpoint_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ef3f358-3729-4919-8a93-63c1457c54ae",
      "metadata": {
        "id": "0ef3f358-3729-4919-8a93-63c1457c54ae"
      },
      "outputs": [],
      "source": [
        "# Define a list of callbacks to be used during training:\n",
        "# 1. EarlyStopping: Stops training when the validation loss stops improving, with a patience of 10 epochs.\n",
        "#    It also restores the best weights of the model when training is stopped.\n",
        "# 2. ModelCheckpoint: Saves the model at each epoch if the validation loss improves.\n",
        "#    The model is saved to the specified 'checkpoint_path', and both the architecture and weights are saved.\n",
        "callback = [\n",
        "    keras.callbacks.EarlyStopping(\n",
        "        monitor='val_loss',              # Monitor validation loss for improvements\n",
        "        patience=10,                     # Number of epochs with no improvement to wait before stopping\n",
        "        verbose=1,                       # Print additional logs\n",
        "        restore_best_weights=True        # Restore the best weights found during training\n",
        "    ),\n",
        "    tf.keras.callbacks.ModelCheckpoint(\n",
        "        filepath=checkpoint_path,        # Path to save the model\n",
        "        monitor='val_loss',              # Metric to monitor for saving the best model\n",
        "        verbose=1,                       # Print additional logs\n",
        "        save_best_only=True,             # Save only if the monitored metric ('val_loss') has improved\n",
        "        save_weights_only=False,         # Save the entire model, not just the weights\n",
        "        mode='auto',                     # Automatically infer the direction of monitoring (min or max)\n",
        "        save_freq='epoch',               # Save the model at the end of every epoch\n",
        "        options=None,                    # Additional options (None in this case)\n",
        "        initial_value_threshold=None     # Threshold for initial value (None in this case)\n",
        "    )\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8eadbca-087a-4070-a7be-fbfffe88f276",
      "metadata": {
        "id": "b8eadbca-087a-4070-a7be-fbfffe88f276"
      },
      "outputs": [],
      "source": [
        "# Define the Nadam optimizer with specific hyperparameters:\n",
        "# - learning_rate: The step size for updating the model's weights, set to 0.001\n",
        "# - beta_1: The exponential decay rate for the first moment estimates, set to 0.9\n",
        "# - beta_2: The exponential decay rate for the second moment estimates, set to 0.999\n",
        "# - epsilon: A small constant to prevent division by zero in the Adam calculations, set to 1e-07\n",
        "optimizer = tf.keras.optimizers.Nadam(\n",
        "    learning_rate=0.001,\n",
        "    beta_1=0.9,\n",
        "    beta_2=0.999,\n",
        "    epsilon=1e-07\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49a26bd9-a407-4054-8fba-8cae62457e5f",
      "metadata": {
        "id": "49a26bd9-a407-4054-8fba-8cae62457e5f"
      },
      "outputs": [],
      "source": [
        "# Compile the model with the specified loss function, optimizer, and evaluation metric:\n",
        "# - Loss function: 'categorical_crossentropy', suitable for multi-class classification problems\n",
        "# - Optimizer: The previously defined Nadam optimizer\n",
        "# - Metrics: 'accuracy', to monitor the classification accuracy during training and validation\n",
        "model.compile(\n",
        "    loss='categorical_crossentropy',\n",
        "    optimizer=optimizer,\n",
        "    metrics=['accuracy']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e8ffdfe-9b26-40d3-90b6-9026e5bd3820",
      "metadata": {
        "id": "2e8ffdfe-9b26-40d3-90b6-9026e5bd3820",
        "outputId": "08bda1e8-79be-4a12-db01-5465f3d68400",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-08-07 16:56:40.032785: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8201\n",
            "2023-08-07 16:56:42.209969: I tensorflow/stream_executor/cuda/cuda_blas.cc:1774] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "208/208 [==============================] - ETA: 0s - loss: 0.1212 - accuracy: 0.9630\n",
            "Epoch 00001: val_loss improved from inf to 0.45539, saving model to best_model.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/apps/tensorflow/2.7.0/lib/python3.9/site-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  layer_config = serialize_layer_fn(layer)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "208/208 [==============================] - 31s 86ms/step - loss: 0.1212 - accuracy: 0.9630 - val_loss: 0.4554 - val_accuracy: 0.8748\n",
            "Epoch 2/50\n",
            "208/208 [==============================] - ETA: 0s - loss: 0.0479 - accuracy: 0.9898\n",
            "Epoch 00002: val_loss improved from 0.45539 to 0.24021, saving model to best_model.h5\n",
            "208/208 [==============================] - 15s 72ms/step - loss: 0.0479 - accuracy: 0.9898 - val_loss: 0.2402 - val_accuracy: 0.9290\n",
            "Epoch 3/50\n",
            "208/208 [==============================] - ETA: 0s - loss: 0.0537 - accuracy: 0.9825\n",
            "Epoch 00003: val_loss did not improve from 0.24021\n",
            "208/208 [==============================] - 14s 69ms/step - loss: 0.0537 - accuracy: 0.9825 - val_loss: 4.6525 - val_accuracy: 0.6149\n",
            "Epoch 4/50\n",
            "208/208 [==============================] - ETA: 0s - loss: 0.0742 - accuracy: 0.9777\n",
            "Epoch 00004: val_loss improved from 0.24021 to 0.16874, saving model to best_model.h5\n",
            "208/208 [==============================] - 15s 72ms/step - loss: 0.0742 - accuracy: 0.9777 - val_loss: 0.1687 - val_accuracy: 0.9410\n",
            "Epoch 5/50\n",
            "208/208 [==============================] - ETA: 0s - loss: 0.0405 - accuracy: 0.9880\n",
            "Epoch 00005: val_loss did not improve from 0.16874\n",
            "208/208 [==============================] - 14s 69ms/step - loss: 0.0405 - accuracy: 0.9880 - val_loss: 1.4301 - val_accuracy: 0.7425\n",
            "Epoch 6/50\n",
            "207/208 [============================>.] - ETA: 0s - loss: 0.0531 - accuracy: 0.9831\n",
            "Epoch 00006: val_loss improved from 0.16874 to 0.07160, saving model to best_model.h5\n",
            "208/208 [==============================] - 15s 74ms/step - loss: 0.0530 - accuracy: 0.9831 - val_loss: 0.0716 - val_accuracy: 0.9795\n",
            "Epoch 7/50\n",
            "208/208 [==============================] - ETA: 0s - loss: 0.0169 - accuracy: 0.9958\n",
            "Epoch 00007: val_loss improved from 0.07160 to 0.05473, saving model to best_model.h5\n",
            "208/208 [==============================] - 15s 74ms/step - loss: 0.0169 - accuracy: 0.9958 - val_loss: 0.0547 - val_accuracy: 0.9856\n",
            "Epoch 8/50\n",
            "208/208 [==============================] - ETA: 0s - loss: 0.0197 - accuracy: 0.9931\n",
            "Epoch 00008: val_loss did not improve from 0.05473\n",
            "208/208 [==============================] - 15s 71ms/step - loss: 0.0197 - accuracy: 0.9931 - val_loss: 0.3625 - val_accuracy: 0.9073\n",
            "Epoch 9/50\n",
            "208/208 [==============================] - ETA: 0s - loss: 0.0337 - accuracy: 0.9898\n",
            "Epoch 00009: val_loss did not improve from 0.05473\n",
            "208/208 [==============================] - 15s 71ms/step - loss: 0.0337 - accuracy: 0.9898 - val_loss: 0.4467 - val_accuracy: 0.8748\n",
            "Epoch 10/50\n",
            "208/208 [==============================] - ETA: 0s - loss: 0.0658 - accuracy: 0.9765\n",
            "Epoch 00010: val_loss did not improve from 0.05473\n",
            "208/208 [==============================] - 14s 69ms/step - loss: 0.0658 - accuracy: 0.9765 - val_loss: 0.9297 - val_accuracy: 0.8303\n",
            "Epoch 11/50\n",
            "207/208 [============================>.] - ETA: 0s - loss: 0.0856 - accuracy: 0.9743\n",
            "Epoch 00011: val_loss did not improve from 0.05473\n",
            "208/208 [==============================] - 14s 69ms/step - loss: 0.0854 - accuracy: 0.9744 - val_loss: 5.4325 - val_accuracy: 0.8616\n",
            "Epoch 12/50\n",
            "208/208 [==============================] - ETA: 0s - loss: 0.0253 - accuracy: 0.9922\n",
            "Epoch 00012: val_loss did not improve from 0.05473\n",
            "208/208 [==============================] - 14s 69ms/step - loss: 0.0253 - accuracy: 0.9922 - val_loss: 0.0903 - val_accuracy: 0.9699\n",
            "Epoch 13/50\n",
            "208/208 [==============================] - ETA: 0s - loss: 0.0214 - accuracy: 0.9952\n",
            "Epoch 00013: val_loss did not improve from 0.05473\n",
            "208/208 [==============================] - 14s 69ms/step - loss: 0.0214 - accuracy: 0.9952 - val_loss: 0.2645 - val_accuracy: 0.9085\n",
            "Epoch 14/50\n",
            "208/208 [==============================] - ETA: 0s - loss: 0.1031 - accuracy: 0.9738\n",
            "Epoch 00014: val_loss did not improve from 0.05473\n",
            "208/208 [==============================] - 15s 71ms/step - loss: 0.1031 - accuracy: 0.9738 - val_loss: 3.6716 - val_accuracy: 0.5764\n",
            "Epoch 15/50\n",
            "207/208 [============================>.] - ETA: 0s - loss: 0.0300 - accuracy: 0.9900\n",
            "Epoch 00015: val_loss did not improve from 0.05473\n",
            "208/208 [==============================] - 14s 69ms/step - loss: 0.0299 - accuracy: 0.9901 - val_loss: 0.2502 - val_accuracy: 0.9362\n",
            "Epoch 16/50\n",
            "208/208 [==============================] - ETA: 0s - loss: 0.0077 - accuracy: 0.9982\n",
            "Epoch 00016: val_loss did not improve from 0.05473\n",
            "208/208 [==============================] - 14s 69ms/step - loss: 0.0077 - accuracy: 0.9982 - val_loss: 0.1484 - val_accuracy: 0.9567\n",
            "Epoch 17/50\n",
            "208/208 [==============================] - ETA: 0s - loss: 0.0021 - accuracy: 1.0000Restoring model weights from the end of the best epoch: 7.\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.05473\n",
            "208/208 [==============================] - 15s 71ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.1042 - val_accuracy: 0.9747\n",
            "Epoch 00017: early stopping\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x2b9f55ef5e50>"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Train the model using the fit method:\n",
        "# - x_train and t_train: The training data and corresponding labels\n",
        "# - batch_size: The number of samples per gradient update, set to 16\n",
        "# - epochs: The number of times to iterate over the entire training dataset, set to 50\n",
        "# - validation_data: The validation data and corresponding labels, used to evaluate the model after each epoch\n",
        "# - callbacks: A list of callbacks to be applied during training, including EarlyStopping and ModelCheckpoint\n",
        "model.fit(\n",
        "    x_train, t_train,\n",
        "    batch_size=16,\n",
        "    epochs=50,\n",
        "    validation_data=(x_valid, t_valid),\n",
        "    callbacks=[callback]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd963198-ae27-43aa-b875-ae3a340610b5",
      "metadata": {
        "id": "fd963198-ae27-43aa-b875-ae3a340610b5"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Tensorflow-2.7.0",
      "language": "python",
      "name": "tensorflow-2.7.0"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
